## 一、线性回归

<font color=green>预测值 $h_{\theta}(x)$</font>：

$$
\begin{align*}
h_{\theta}(x)
&= \theta_0 \ + \ \theta_1x_1 \ + \ \theta_2x_2 + \dots \\
&= \sum_{i=0}^{n}\theta_ix_i\\
&= \theta^Tx \\
\end{align*}
$$

上式中 $\theta_0$ 通常被称为<font color=green>偏置值</font>。

<font color=green>误差 $\varepsilon$</font>：真实值 $y^{(i)}$ 与预测值 $h_{\theta}(x)$ 之间的差异。误差 $\varepsilon^{(i)}$ 是独立同分布且 $\varepsilon^{(i)} \sim N(0,\ \theta^2)$

对于每一个样本有：

$$
y^{(i)} = \theta^Tx^{(i)} \ + \ \varepsilon^{(i)} \tag 1 \\
$$

显然，我们的目标是使 $|\varepsilon^{(i)}|$ 尽可能小。

又因为有 $\varepsilon^{(i)} \sim N(0,\ \theta^2)$，即 

$$
p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(\varepsilon^{(i)})^2}{2\sigma^2}) \tag 2 \\
$$

欲使 $|\varepsilon^{(i)}|$ 尽可能小，等价于使得 $|\varepsilon^{(i)}|$ 尽可能接近于 $0$，也就是说使的概率 $p(\varepsilon^{(i)})$ 尽可能大（因为均值为 $0$ 的正态函数在 $\varepsilon = 0$ 的时候取最大值）。

将 $(1)$ 式带入 $(2)$ 中有：

$$
p(y^{(i)}\ |\ x^{(i)}; \ \theta) = \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^Tx^{(i)}))^2}{2\sigma^2})
$$

上式左边表是当 $\theta$ 与 $x^{(i)}$ 组合之后所求出的预测值 $h_\theta(x)$ 与真实值 $y^{(i)}$ 之间的误差 $\varepsilon^{(i)}$ 应该越小，即概率 $p(y^{(i)}\ |\ x^{(i)}; \ \theta)$ 越大越好，也就是说求一个 $\theta$ 使得概率 $p(y^{(i)}\ |\ x^{(i)}; \ \theta)$ 尽可能的大。这里就可以转换为概率统计中的最大似然函数来求解。

似然函数：


$$
\begin{align*}
L(\theta) &= \prod_{i=1}^m p(y^{(i)}\ |\ x^{(i)}; \ \theta) \\
&= \prod_{i=1}^m \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^Tx^{(i)}))^2}{2\sigma^2})
\end{align*}
$$


根据最大似然函数的求解方法，两边同时取 $ln$ 有

$$
\begin{align*}
ln L(\theta) &= ln \prod_{i=1}^m \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^Tx^{(i)}))^2}{2\sigma^2}) \\
&= mln\frac{1}{\sqrt{2 \pi} \sigma} - \frac{1}{\sigma^2} \dot \ \frac{1}{2} \sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)}))^2 \\
J(\theta) &\overset{\Delta}{=} \frac{1}{2} \sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)}))^2 \ \ \text{(最小二乘法)} \\
\end{align*}
$$

有单调性，要使 $L(\theta)$ 最大，就需要使得 $J(\theta)$ 最小，

<font color=green>目标函数/损失函数 $J(\theta)$</font>：

$$
\begin{align*}
J(\theta) &= \frac{1}{2} \sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)}))^2\\
&= \frac{1}{2} (X\theta - y)^T(X\theta - y)
\end{align*}
$$

求偏导：

$$
\begin{align}
\nabla_\theta J(\theta) &= \nabla_\theta (\frac{1}{2} (X\theta - y)^T(X\theta - y)) \\
&=\nabla_\theta(\frac{1}{2} (\theta^T X^T - y^T) (X\theta-y)) \\ 
&=\nabla_\theta(\frac{1}{2} (\theta^TX^TX\theta - \theta^TX^Ty - y^T X \theta + y^Ty)) \\
&= \frac{1}{2} (2 X^T X \theta - X^T y - (y^T X)^T) \\ 
&= X^T X \theta - X^T y \\
&\overset{\Delta}{=} 0 \\
\Rightarrow &\ \theta = (X^TX)^{-1}X^Ty
\end{align}
$$

> 对上式中 $(3)$ 到 $(4)$ 的过程： 线性代数中 $X^TX$ 的结果一定是一个对称矩阵，对于一个对称阵 $A$，有结论 $\frac{\partial \theta^T A \theta}{\partial \theta} = 2 A \theta$。

到这里就存在两个问题：

1. 由上述公式，对于给定的数据矩阵 $X$ 和 $y$，就可以直接求出 $\theta$，那么机器学习的过程呢？
2. 对于矩阵 $X$ 不一定总是可逆矩阵。

重新构造目标函数：$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (y^{i} - \theta^Tx^{i})^2$

> 上式中分母加上了 $m$ 是为了取到尽可能多的样本的平均值，后面加上平方是为了防止符号。

1、批量梯度下降：

$$
\begin{align*}
\frac{\partial J(\theta)}{\partial \theta_j} &= -\frac{1}{m} \sum_{i=1}^{m}(y^{i} - h_\theta(x^{i}))x_j^i \tag 1 \\
\theta_j^{‘} &= \theta + \frac{1}{m} \sum_{i=1}^{m}(y^{i} - h_\theta(x^{i}))x_j^i \tag 2 \\
\end{align*}
$$

> - 上式 $(2)$ 中更新后的 $\theta_j^{'}$ 应该是原来的 $\theta$ 减去步长 $-\frac{1}{m} \sum_{i=1}^{m}(y^{i} - h_\theta(x^{i}))x_j^i \tag 2$，因为是朝梯度下降的方向走。
> - 上式计算容易得到最优解，但是由于每次都需要考虑所有样本，速度很慢

2、随机梯度下降：$\theta_j^{‘} = \theta + (y^{i} - h_\theta(x^{i}))x_j^i$

> 随机梯度下降每一次随机找一个样本，迭代速度很快，但是不一定每次都朝着收敛的方向

3、小批量梯度下降：

$$
\theta_j \ := \ \theta_j - \alpha\frac{1}{10} \sum_{k=i}^{i+9} (h_\theta(x^{(k)}) - y^{(k)}) x_j^{(k)}
$$

> - 每次选择一下部分数据来更新
> - <font color=green>学习率/步长 $\alpha$</font>：对结果会产生巨大的影响，一般从小开始往大设置。

python机器学习工具包简介：`sckkit-learn`



## 二、逻辑回归

经典用法：解决二分类问题。

### 2.1 <font color=green>sigmoid</font> 函数：

- 公式：$g(z) = \frac{1}{1 + e^{-z}}$
- 自变量取值为任意实数，值域为 $[0,\ 1]$

![](https://cdn.jsdelivr.net/gh/LuciferCCC/blogs_images@main/20230202203851.png)

### 2.2 逻辑回归公式原理

__预测函数__：

$$
h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}} \\
$$

其中 $\theta^Tx = \theta_0 + \theta_1x_1 + , \dots, + \theta_nx_n = \sum_{i=1}^n \theta_i x_i$

__分类任务__：

$$
\begin{align*}
&P(y=1|x;\theta) = h_theta(x) \\
&P(y=0|x;\theta) = 1 - h_theta(x) \\
\overset{整合}\Rightarrow  &P(y|x;\theta) = (h_\theta(x))^y(1 - h_\theta(x))^{1-y}
\end{align*}
$$

__似然函数__：

$$
L(\theta) = \prod_{i=1}^m P(y_i | x_i; \theta) = \prod_{i=1}^m (h_\theta(x))^y(1 - h_\theta(x))^{1-y}
$$


__对数似然函数__：

$$
l(\theta) = logL(\theta) = \sum_{i=1}^m (y_i logh_\theta(x_i) + (1 - y_i)log(1 - h_\theta(x_i)))
$$


这时候是一个梯度上升问题，要使的 $L(\theta)$ 最大，应该求 $\theta$ 的最大值。

现在引入 $-$ 转换为下降任务。

$$
J(\theta) = - \frac{1}{m} l(\theta)
$$

$J(\theta)$ 对 $\theta$ 求求解偏导后可以得到 

$$
-\frac{1}{m} \sum_{i=1}^{m} (y_i - g(\theta^T x_i)) x_i^j
$$

__参数更新__：

$$
\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m(h_\theta(x_i) - y_i)x_i^j
$$
