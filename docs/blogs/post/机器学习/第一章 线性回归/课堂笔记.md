<font color=green>预测值 $h_{\theta}(x)$</font>：
$$
\begin{align*}
h_{\theta}(x)
&= \theta_0 \ + \ \theta_1x_1 \ + \ \theta_2x_2 + \dots \\
&= \sum_{i=0}^{n}\theta_ix_i\\
&= \theta^Tx
\end{align*}
$$
上式中 $\theta_0$ 通常被称为<font color=green>偏置值</font>。

<font color=green>误差 $\varepsilon$</font>：真实值 $y^{(i)}$ 与预测值 $h_{\theta}(x)$ 之间的差异。误差 $\varepsilon^{(i)}$ 是独立同分布且 $\varepsilon^{(i)} \sim N(0,\ \theta^2)$

对于每一个样本有：
$$
y^{(i)} = \theta^Tx^{(i)} \ + \ \varepsilon^{(i)} \tag 1
$$
显然，我们的目标是使 $|\varepsilon^{(i)}|$ 尽可能小。

又因为有 $\varepsilon^{(i)} \sim N(0,\ \theta^2)$，即 
$$
p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(\varepsilon^{(i)})^2}{2\sigma^2}) \tag 2
$$
欲使 $|\varepsilon^{(i)}|$ 尽可能小，等价于使得 $|\varepsilon^{(i)}|$ 尽可能接近于 $0$，也就是说使的概率 $p(\varepsilon^{(i)})$ 尽可能大（因为均值为 $0$ 的正态函数在 $\varepsilon = 0$ 的时候取最大值）。

将 $(1)$ 式带入 $(2)$ 中有：
$$
p(y^{(i)}\ |\ x^{(i)}; \ \theta) = \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^Tx^{(i)}))^2}{2\sigma^2})
$$
上式左边表是当 $\theta$ 与 $x^{(i)}$ 组合之后所求出的预测值 $h_\theta(x)$ 与真实值 $y^{(i)}$ 之间的误差 $\varepsilon^{(i)}$ 应该越小，即概率 $p(y^{(i)}\ |\ x^{(i)}; \ \theta)$ 越大越好，也就是说求一个 $\theta$ 使得概率 $p(y^{(i)}\ |\ x^{(i)}; \ \theta)$ 尽可能的大。这里就可以转换为概率统计中的最大似然函数来求解。

似然函数：
$$
\begin{align*}
L(\theta) &= \prod_{i=1}^m p(y^{(i)}\ |\ x^{(i)}; \ \theta) \\
&= \prod_{i=1}^m \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^Tx^{(i)}))^2}{2\sigma^2})
\end{align*}
$$
根据最大似然函数的求解方法，两边同时取 $ln$ 有
$$
\begin{align*}
ln L(\theta) &= ln \prod_{i=1}^m \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^Tx^{(i)}))^2}{2\sigma^2}) \\

&= mln\frac{1}{\sqrt{2 \pi} \sigma} - \frac{1}{\sigma^2} \dot \ \frac{1}{2} \sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)}))^2 \\

J(\theta) &\overset{\Delta}{=} \frac{1}{2} \sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)}))^2 \ \ \text{(最下二乘法)}
\end{align*}
$$
有单调性，要使 $L(\theta)$ 最大，就需要使得 $J(\theta)$ 最小，

<font color=green>目标函数/损失函数 $J(\theta)$</font>：
$$
\begin{align*}
J(\theta) &= \frac{1}{2} \sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)}))^2\\

&= \frac{1}{2} (X\theta - y)^T(X\theta - y)
\end{align*}
$$
求偏导：
$$
\begin{align}
\nabla_\theta J(\theta) &= \nabla_\theta (\frac{1}{2} (X\theta - y)^T(X\theta - y)) \\

&=\nabla_\theta(\frac{1}{2} (\theta^T X^T - y^T) (X\theta-y)) \\ 

&=\nabla_\theta(\frac{1}{2} (\theta^TX^TX\theta - \theta^TX^Ty - y^T X \theta + y^Ty)) \\

&= \frac{1}{2} (2 X^T X \theta - X^T y - (y^T X)^T) \\ 

&= X^T X \theta - X^T y \\

&\overset{\Delta}{=} 0 \\

\Rightarrow &\ \theta = (X^TX)^{-1}X^Ty
\end{align}
$$

> 对上式中 $(3)$ 到 $(4)$ 的过程： 线性代数中 $X^TX$ 的结果一定是一个对称矩阵，对于一个对称阵 $A$，有结论 $\frac{\partial \theta^T A \theta}{\partial \theta} = 2 A \theta$。

到这里就存在两个问题：

1. 由上述公式，对于给定的数据矩阵 $X$ 和 $y$，就可以直接求出 $\theta$，那么机器学习的过程呢？
2. 对于矩阵 $X$ 不一定总是可逆矩阵。

重新构造目标函数：$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (y^{i} - \theta^Tx^{i})^2$

> 上式中分母加上了 $m$ 是为了取到尽可能多的样本的平均值，后面加上平方是为了防止符号。

1. 批量梯度下降：

$$
\begin{align*}
\frac{\partial J(\theta)}{\partial \theta_j} &= -\frac{1}{m} \sum_{i=1}^{m}(y^{i} - h_\theta(x^{i}))x_j^i \tag 1 \\
\theta_j^{‘} &= \theta + \frac{1}{m} \sum_{i=1}^{m}(y^{i} - h_\theta(x^{i}))x_j^i \tag 2 \\
\end{align*}
$$

> - 上式 $(2)$ 中更新后的 $\theta_j^{'}$ 应该是原来的 $\theta$ 减去步长 $-\frac{1}{m} \sum_{i=1}^{m}(y^{i} - h_\theta(x^{i}))x_j^i \tag 2$，因为是朝梯度下降的方向走。
> - 上式计算容易得到最优解，但是由于每次都需要考虑所有样本，速度很慢

2. 随机梯度下降：$\theta_j^{‘} = \theta + (y^{i} - h_\theta(x^{i}))x_j^i $

    > 随机梯度下降每一次随机找一个样本，迭代速度很快，但是不一定每次都朝着收敛的方向

3. 小批量梯度下降：
    $$
    \theta_j \ := \ \theta_j - \alpha\frac{1}{10} \sum_{k=i}^{i+9} (h_\theta(x^{(k)}) - y^{(k)}) x_j^{(k)}
    $$

    > - 每次选择一下部分数据来更新
    > - <font color=green>学习率/步长 $\alpha$</font>：对结果会产生巨大的影响，一般从小开始往大设置。

python机器学习工具包简介：sckkit-learn
